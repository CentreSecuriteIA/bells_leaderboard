# BELLS - Benchmark for the Evaluation of LLM Safeguards

BELLS is a comprehensive benchmark designed to evaluate the effectiveness of safety measures in Large Language Models (LLMs). Our leaderboard tracks and compares different LLM systems based on their performance across various safety metrics.

## Features

- Interactive leaderboard displaying safety performance metrics
- Detailed breakdown of evaluation criteria
- Playground environment for testing LLM safeguards
- Recommendation system for safety improvements
- Comprehensive metrics documentation

## Getting Started

Visit our live leaderboard at [https://centresecuriteia.github.io/bells_leaderboard/](https://centresecuriteia.github.io/bells_leaderboard/) to:
- View current rankings of LLM systems
- Explore detailed safety metrics
- Test your own LLM system
- Get safety improvement recommendations

## Contributing

We welcome contributions from the AI safety community. Please see our [Contributing Guidelines](CONTRIBUTING.md) for more information on how to:
- Submit your LLM system for evaluation
- Propose new safety metrics
- Report issues or suggest improvements

## Documentation

For detailed information about our evaluation metrics and methodology, please visit our [Metrics Description](metrics.html) page.

## Contact

For questions or support, please [open an issue](https://github.com/your-repo/bells/issues) or contact us at [email].

