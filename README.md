# BELLS - Benchmark for the Evaluation of LLM Supervision Systems

BELLS is a comprehensive research benchmark designed to evaluate the effectiveness of supervision systems in Large Language Models (LLMs). Our research report ranks and analyzes different LLM supervision systems based on their performance across various safety metrics, revealing key insights about their capabilities and limitations.

## Research Findings

- Comparative analysis of supervision systems across harm severity levels
- Assessment of resistance to adversarial attacks (jailbreaks)
- Analysis of metacognitive coherence in LLM safety responses
- Evaluation of frontier models vs. specialized supervision systems

## Explore Our Research

Visit our research site at [https://centresecuriteia.github.io/bells_leaderboard/](https://centresecuriteia.github.io/bells_leaderboard/) to:
- Review comparative rankings of LLM supervision systems
- Examine detailed performance analyses across safety dimensions
- Explore our dataset through the interactive playground
- Understand the implications for LLM safety development

## Methodology

For detailed information about our evaluation methodology and metrics, please visit our [Metrics Description](https://centresecuriteia.github.io/bells_leaderboard/metrics.html) page.

## Contact

For questions about our research or methodology, please [open an issue](https://github.com/centresecuriteia/bells_leaderboard/issues)

