# BELLS - Benchmark for the Evaluation of LLM Safeguards

BELLS is a comprehensive benchmark designed to evaluate the effectiveness of safety measures in Large Language Models (LLMs) safeguards and systems. Our leaderboard tracks and compares different LLM safeguards and systems based on their performance across various safety metrics.

## Features

- Interactive leaderboard displaying safeguards performance metrics
- Detailed breakdown of evaluation criteria
- Playground environment for deep diving into our data
- Safeguard recommendation system
- Comprehensive metrics documentation

## Getting Started

Visit our live leaderboard at [https://centresecuriteia.github.io/bells_leaderboard/](https://centresecuriteia.github.io/bells_leaderboard/) to:
- View current rankings of LLM systems
- Explore detailed safety metrics
- Test your own LLM system
- Get safety improvement recommendations

## Documentation

For detailed information about our evaluation metrics and methodology, please visit our [Metrics Description](https://centresecuriteia.github.io/bells_leaderboard/metrics.html) page.

## Contact

For questions or support, please [open an issue](https://github.com/centresecuriteia/bells_leaderboard/issues)

